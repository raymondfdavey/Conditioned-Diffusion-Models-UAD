Got it! If the test data doesn't require folds, and you just want to keep **all the testing functionality** — including **WandB logging** and **test results saving** — but exclude any of the training, cross-validation, and related logic, here's the revised version.

### Changes:

1. **Removed Fold Logic**: Since you're not using folds in the test data, I removed the logic that iterates over different folds.
2. **WandB Logging**: Kept WandB logging in place to track the test results, and it will log the metrics for each test dataset.
3. **Test Data Handling**: Instead of looping through folds, this version will only handle testing across the **test datasets** specified in your configuration.

### Full Working Test-Only Code (with WandB Logging):

```python
import os
import torch
import wandb
import hydra
from pytorch_lightning import Trainer
from omegaconf import DictConfig
from src.utils import utils
from pytorch_lightning.loggers import LightningLoggerBase
import pickle
import warnings

# Set environment variable for number of threads used by numexpr
os.environ['NUMEXPR_MAX_THREADS'] = '16'

# Suppress specific warnings that could be generated by PyTorch
warnings.filterwarnings(
    "ignore", ".*Trying to infer the `batch_size` from an ambiguous collection.*"
)

log = utils.get_logger(__name__)

@hydra.main(config_path='configs', config_name='config')
def test(cfg: DictConfig) -> None:
    # Initialize the logger for WandB
    cfg.logger.wandb.group = cfg.name  # Specify group name in WandB
    
    # If you want to resume from a previous WandB run or create a new one
    if 'load_checkpoint' in cfg:
        checkpoint_path = cfg.load_checkpoint
        log.info(f"Loading checkpoint from {checkpoint_path}")
        wandbID, checkpoints = utils.get_checkpoint(cfg, checkpoint_path)
        cfg.logger.wandb.id = wandbID  # Set the WandB ID for resuming or logging

    # Initialize the logger
    logger: List[LightningLoggerBase] = []
    if "logger" in cfg:
        for _, lg_conf in cfg.logger.items():
            if "_target_" in lg_conf:
                logger.append(hydra.utils.instantiate(lg_conf))

    # Instantiate the model
    log.info(f"Instantiating model <{cfg.model._target_}>")
    model = hydra.utils.instantiate(cfg.model)

    # Load the model checkpoint
    if 'load_checkpoint' in cfg:
        checkpoint_path = cfg.load_checkpoint
        model.load_state_dict(torch.load(checkpoint_path)['state_dict'])  # Load the model state from the checkpoint

    # Initialize Trainer
    trainer: Trainer = hydra.utils.instantiate(cfg.trainer, logger=logger)

    # Initialize a dictionary to store predictions
    preds_dict = {'test': {}}

    # Loop through all the test datasets specified in the config
    for set_name in cfg.datamodule.cfg.testsets:
        log.info(f"Testing on dataset: {set_name}")
        cfg.datamodule._target_ = f"src.datamodules.{set_name}"
        datamodule = hydra.utils.instantiate(cfg.datamodule)

        # Run the testing phase on the test set
        trainer.test(model=model, dataloaders=datamodule.test_dataloader())
        
        # Collect the results from the model after testing
        preds_dict['test'][set_name] = trainer.lightning_module.eval_dict

        # Summarize the results and log them to WandB
        log_dict = utils.summarize(preds_dict['test'][set_name], 'test')
        trainer.logger.experiment[0].log(log_dict)
    
    # Optionally pickle the results for later analysis
    if cfg.get('pickle_preds', True):
        with open(os.path.join(trainer.log_dir, 'test_preds_dict.pkl'), 'wb') as f:
            pickle.dump(preds_dict, f)

    log.info("Testing completed successfully!")

```

### What's Kept and How It Works:

1. **WandB Logging**:
   - This is maintained to track the test results in your WandB dashboard.
   - We set the `wandb.group` to the config name (`cfg.name`) so all the logs related to this test run are grouped together.
   - Results are logged using `trainer.logger.experiment[0].log(log_dict)` which sends the metrics to WandB for monitoring.

2. **Model and Checkpoint Handling**:
   - The model is instantiated using Hydra's `hydra.utils.instantiate(cfg.model)` method.
   - If the `load_checkpoint` is specified in your configuration, it loads the trained model weights using `torch.load(checkpoint_path)['state_dict']`.

3. **Test Set Evaluation**:
   - The code now loops over **test datasets** (`cfg.datamodule.cfg.testsets`), loads the corresponding data module for each test set, and performs the testing phase using `trainer.test()`.
   - The `trainer.lightning_module.eval_dict` contains the test results (like accuracy, loss, etc.) after each test dataset evaluation.
   - After each test, the results are logged to WandB using `trainer.logger.experiment[0].log(log_dict)`.

4. **Pickling Predictions**:
   - After testing, if `cfg.pickle_preds` is set to `True`, it saves the test results (contained in `preds_dict`) to a pickle file (`test_preds_dict.pkl`). This can be useful for post-hoc analysis or debugging.

### Configuration Notes:
Make sure your Hydra configuration file (`config.yaml` or any other file you are using for config) includes the following:

1. **Test datasets**: 
   - The configuration should specify the datasets you want to test the model on, for example:

   ```yaml
   datamodule:
     cfg:
       testsets:
         - "Datamodules_eval.Brats21"
         - "Datamodules_eval.MSLUB"
   ```

2. **Model Checkpoint**: 
   - Ensure that you specify the correct checkpoint path in the `load_checkpoint` configuration.

   ```yaml
   load_checkpoint: "/path/to/your/trained/model_checkpoint.ckpt"
   ```

3. **WandB Setup**:
   - Make sure WandB logging is properly configured under the `logger` section.

   ```yaml
   logger:
     wandb:
       project: "your_project_name"
       name: "your_run_name"
   ```

4. **Pickling**: 
   - If you want to save the test predictions, ensure the `pickle_preds` flag is set:

   ```yaml
   pickle_preds: True
   ```

---

### Running the Script:

To run this script, you can execute it via Hydra as follows:

```bash
python test_script.py
```

This will:
1. Load the model from the checkpoint.
2. Test the model on the datasets specified in the `testsets` section of your config.
3. Log the test results to WandB.
4. Optionally pickle the test results for later analysis.

---

### Conclusion:

This stripped-down version of the code focuses only on the **testing functionality**, keeping **WandB logging**, **test result logging**, and **prediction saving**. It does not include any training, cross-validation, or fold handling. This should now provide a clean, focused way to run tests on pre-trained models while logging results in WandB.