import os
import torch
import wandb
import hydra
from pytorch_lightning import Trainer
from omegaconf import DictConfig
from src.utils import utils
from pytorch_lightning.loggers import LightningLoggerBase
import pickle
import warnings
from typing import List, Optional

# Set environment variable for number of threads used by numexpr
os.environ['NUMEXPR_MAX_THREADS'] = '16'

# Suppress specific warnings that could be generated by PyTorch
warnings.filterwarnings(
    "ignore", ".*Trying to infer the `batch_size` from an ambiguous collection.*"
)

log = utils.get_logger(__name__)

@hydra.main(config_path='configs', config_name='config')
def test(cfg: DictConfig) -> None:
    # Initialize the logger for WandB
    cfg.logger.wandb.group = cfg.name  # Specify group name in WandB
    
    # If you want to resume from a previous WandB run or create a new one
    if 'load_checkpoint' in cfg:
        checkpoint_path = cfg.load_checkpoint
        log.info(f"Loading checkpoint from {checkpoint_path}")
        wandbID, checkpoints = utils.get_checkpoint(cfg, checkpoint_path)
        cfg.logger.wandb.id = wandbID  # Set the WandB ID for resuming or logging

    # Initialize the logger
    logger: List[LightningLoggerBase] = []
    if "logger" in cfg:
        for _, lg_conf in cfg.logger.items():
            if "_target_" in lg_conf:
                logger.append(hydra.utils.instantiate(lg_conf))

    # Instantiate the model
    log.info(f"Instantiating model <{cfg.model._target_}>")
    model = hydra.utils.instantiate(cfg.model)

    # Load the model checkpoint
    if 'load_checkpoint' in cfg:
        checkpoint_path = cfg.load_checkpoint
        model.load_state_dict(torch.load(checkpoint_path)['state_dict'])  # Load the model state from the checkpoint

    # Initialize Trainer
    trainer: Trainer = hydra.utils.instantiate(cfg.trainer, logger=logger)

    # Initialize a dictionary to store predictions
    preds_dict = {'test': {}}

    # Loop through all the test datasets specified in the config
    for set_name in cfg.datamodule.cfg.testsets:
        log.info(f"Testing on dataset: {set_name}")
        cfg.datamodule._target_ = f"src.datamodules.{set_name}"
        datamodule = hydra.utils.instantiate(cfg.datamodule)

        # Run the testing phase on the test set
        trainer.test(model=model, dataloaders=datamodule.test_dataloader())
        
        # Collect the results from the model after testing
        preds_dict['test'][set_name] = trainer.lightning_module.eval_dict

        # Summarize the results and log them to WandB
        log_dict = utils.summarize(preds_dict['test'][set_name], 'test')
        trainer.logger.experiment[0].log(log_dict)
    
    # Optionally pickle the results for later analysis
    if cfg.get('pickle_preds', True):
        with open(os.path.join(trainer.log_dir, 'test_preds_dict.pkl'), 'wb') as f:
            pickle.dump(preds_dict, f)

    log.info("Testing completed successfully!")

